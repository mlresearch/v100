---
title: Mutual-Information Regularization in Markov Decision Processes and Actor-Critic
  Learning
abstract: Cumulative entropy regularization introduces a regulatory signal to the
  reinforcement learning (RL) problem that encourages policies with high-entropy actions,
  which is equivalent to enforcing small deviations from a uniform reference marginal
  policy. This has been shown to improve exploration and robustness, and it tackles
  the value overestimation problem. It also leads to a significant performance increase
  in tabular and high-dimensional settings, as demonstrated via algorithms such as
  soft Q-learning (SQL) and soft actor-critic (SAC). Cumulative entropy regularization
  has been extended to optimize over the reference marginal policy instead of keeping
  it fixed, yielding a regularization that minimizes the mutual information between
  states and actions. While this has been initially proposed for Markov Decision Processes
  (MDPs) in tabular settings, it was recently shown that a similar principle leads
  to significant improvements over vanilla SQL in RL for high-dimensional domains
  with discrete actions and function approximators. Here, we follow the motivation
  of mutual-information regularization from an inference perspective and theoretically
  analyze the corresponding Bellman operator. Inspired by this Bellman operator, we
  devise a novel mutual-information regularized actor-critic learning (MIRACLE) algorithm
  for continuous action spaces that optimizes over the reference marginal policy.
  We empirically validate MIRACLE in the Mujoco robotics simulator, where we demonstrate
  that it can compete with contemporary RL methods. Most notably, it can improve over
  the model-free state-of-the-art SAC algorithm which implicitly assumes a fixed reference
  policy.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: leibfried20a
month: 0
tex_title: Mutual-Information Regularization in Markov Decision Processes and Actor-Critic
  Learning
firstpage: 360
lastpage: 373
page: 360-373
order: 360
cycles: false
bibtex_author: Leibfried, Felix and Grau-Moya, Jordi
author:
- given: Felix
  family: Leibfried
- given: Jordi
  family: Grau-Moya
date: 2020-05-12
address: 
publisher: PMLR
container-title: Proceedings of the Conference on Robot Learning
volume: '100'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 5
  - 12
pdf: http://proceedings.mlr.press/v100/leibfried20a/leibfried20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
