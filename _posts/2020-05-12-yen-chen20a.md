---
title: Experience-Embedded Visual Foresight
abstract: 'Visual foresight gives an agent a window into the future, which it can
  use to anticipate events before they happen and plan strategic behavior. Although
  impressive results have been achieved on video prediction in constrained settings,
  these models fail to generalize when confronted with unfamiliar real-world objects.
  In this paper, we tackle the generalization problem via fast adaptation, where we
  train a prediction model to quickly adapt to the observed visual dynamics of a novel
  object. Our method, Experience-embedded Visual Foresight (EVF), jointly learns a
  fast adaptation module, which encodes observed trajectories of the new object into
  a vector embedding, and a visual prediction model, which conditions on this embedding
  to generate physically plausible predictions. For evaluation, we compare our method
  against baselines on video prediction and benchmark its utility on two real world
  control tasks. We show that our method is able to quickly adapt to new visual dynamics
  and achieves lower error than the baselines when manipulating novel objects. Videos
  are available at: http://evf.csail.mit.edu/.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yen-chen20a
month: 0
tex_title: Experience-Embedded Visual Foresight
firstpage: 1015
lastpage: 1024
page: 1015-1024
order: 1015
cycles: false
bibtex_author: Yen-Chen, Lin and Bauza, Maria and Isola, Phillip
author:
- given: Lin
  family: Yen-Chen
- given: Maria
  family: Bauza
- given: Phillip
  family: Isola
date: 2020-05-12
address: 
publisher: PMLR
container-title: Proceedings of the Conference on Robot Learning
volume: '100'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 5
  - 12
pdf: http://proceedings.mlr.press/v100/yen-chen20a/yen-chen20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
