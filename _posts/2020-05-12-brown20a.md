---
title: Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations
abstract: The performance of imitation learning is typically upper-bounded by the
  performance of the demonstrator. While recent empirical results demonstrate that
  ranked demonstrations allow for better-than-demonstrator performance, preferences
  over demonstrations may be difficult to obtain, and little is known theoretically
  about when such methods can be expected to successfully extrapolate beyond the performance
  of the demonstrator. To address these issues, we first contribute a sufficient condition
  for better-than-demonstrator imitation learning and provide theoretical results
  showing why preferences over demonstrations can better reduce reward function ambiguity
  when performing inverse reinforcement learning. Building on this theory, we introduce
  Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning
  method that injects noise into a policy learned through behavioral cloning to automatically
  generate ranked demonstrations. These ranked demonstrations are used to efficiently
  learn a reward function that can then be optimized using reinforcement learning.
  We empirically validate our approach on simulated robot and Atari imitation learning
  benchmarks and show that D-REX outperforms standard imitation learning approaches
  and can significantly surpass the performance of the demonstrator. D-REX is the
  first imitation learning approach to achieve significant extrapolation beyond the
  demonstratorâ€™s performance without additional side-information or supervision, such
  as rewards or human preferences. By generating rankings automatically, we show that
  preference-based inverse reinforcement learning can be applied in traditional imitation
  learning settings where only unlabeled demonstrations are available.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: brown20a
month: 0
tex_title: Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations
firstpage: 330
lastpage: 359
page: 330-359
order: 330
cycles: false
bibtex_author: Brown, Daniel S. and Goo, Wonjoon and Niekum, Scott
author:
- given: Daniel S.
  family: Brown
- given: Wonjoon
  family: Goo
- given: Scott
  family: Niekum
date: 2020-05-12
address: 
publisher: PMLR
container-title: Proceedings of the Conference on Robot Learning
volume: '100'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 5
  - 12
pdf: http://proceedings.mlr.press/v100/brown20a/brown20a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v100/brown20a/brown20a-supp.zip
- label: Website
  link: https://dsbrown1331.github.io/CoRL2019-DREX/
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
