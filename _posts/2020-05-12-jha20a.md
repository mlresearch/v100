---
title: Quasi-Newton Trust Region Policy Optimization
abstract: 'We propose a trust region method for policy optimization that employs Quasi-Newton
  approximation for the Hessian, called Quasi-Newton Trust Region Policy Optimization
  (QNTRPO). Gradient descent is the de facto algorithm for reinforcement learning
  tasks with continuous controls. The algorithm has achieved state-of-the-art performance
  when used in reinforcement learning across a wide range of tasks. However, the algorithm
  suffers from a number of drawbacks including: lack of stepsize selection criterion,
  and slow convergence. We investigate the use of a trust region method using dogleg
  step and a Quasi-Newton approximation for the Hessian for policy optimization. We
  demonstrate through numerical experiments over a wide range of challenging continuous
  control tasks that our particular choice is efficient in terms of number of samples
  and improves performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jha20a
month: 0
tex_title: Quasi-Newton Trust Region Policy Optimization
firstpage: 945
lastpage: 954
page: 945-954
order: 945
cycles: false
bibtex_author: Jha, Devesh K. and Raghunathan, Arvind U. and Romeres, Diego
author:
- given: Devesh K.
  family: Jha
- given: Arvind U.
  family: Raghunathan
- given: Diego
  family: Romeres
date: 2020-05-12
address: 
publisher: PMLR
container-title: Proceedings of the Conference on Robot Learning
volume: '100'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 5
  - 12
pdf: http://proceedings.mlr.press/v100/jha20a/jha20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v100/jha20a/jha20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
