---
title: Dynamic Experience Replay
abstract: 'We present a novel technique called Dynamic Experience Replay (DER) that
  allows Reinforcement Learning (RL) algorithms to use experience replay samples not
  only from human demonstrations but also successful transitions generated by RL agents
  during training and therefore improve training efficiency. It can be combined with
  an arbitrary off-policy RL algorithm, such as DDPG or DQN, and their distributed
  versions.We build upon Ape-X DDPG and demonstrate our approach on robotic tight-fitting
  joint assembly tasks, based on force/torque and Cartesian pose observations. In
  particular, we run experiments on two different tasks: peg-in-hole and lap-joint.
  In each case, we compare different replay buffer structures and how DER affects
  them. Our ablation studies show that Dynamic Experience Replay is a crucial ingredient
  that either largely shortens the training time in these challenging environments
  or solves the tasks that the vanilla Ape-X DDPG cannot solve. We also show that
  our policies learned purely in simulation can be deployed successfully on the real
  robot. The video presenting our experiments is available at https://sites.google.com/site/dynamicexperiencereplay'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: luo20a
month: 0
tex_title: Dynamic Experience Replay
firstpage: 1191
lastpage: 1200
page: 1191-1200
order: 1191
cycles: false
bibtex_author: Luo, Jieliang and Li, Hui
author:
- given: Jieliang
  family: Luo
- given: Hui
  family: Li
date: 2020-05-12
address: 
publisher: PMLR
container-title: Proceedings of the Conference on Robot Learning
volume: '100'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 5
  - 12
pdf: http://proceedings.mlr.press/v100/luo20a/luo20a.pdf
extras:
- label: Supplementary video
  link: https://sites.google.com/site/dynamicexperiencereplay
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
